I"&Z<blockquote>
  <p>We believe that deep learning has the ability to revolutionize healthcare. Read our <a href="https://blog.qure.ai/notes/on-qure-ai">post</a> on deep learning in healthcare to understand where we are headed. Reaching this goal will require contributions from many people, both within and outside of Qure.ai. In this spirit, we want to open-source our work, wherever possible, so that the super-talented global deep learning community can build upon our solutions. In this post, we share our work on segmentation of nerves from the ultrasound images.</p>
</blockquote>

<p><a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation">Kaggle ultrasound nerve segmentation challenge</a> is one of the high profile challenges hosted on Kaggle. We have used U-net neural network architecture and <a href="https://github.com/torchnet/torchnet">torchnet</a> package for tackling the challenge and achieved some remarkable results. The repository can be found <a href="https://github.com/qureai/ultrasound-nerve-segmentation-using-torchnet">here</a>.</p>

<p>The challenge in itself is a great learning experience for segmentation problems.
Figure below is an example of the image and the mask to predict.</p>

<p align="center">
    <img src="/assets/images/ultrasound_torchnet/1_1.jpg" alt="Image" style="float:left;width:48%;margin:1%;margin-bottom:2em;" />
    <img src="/assets/images/ultrasound_torchnet/1_1_mask.jpg" alt="Mask" style="float:right;width:48%;margin:1%;margin-bottom:2em" />
</p>

<h2 id="requirements">Requirements</h2>

<p>We assume following are installed in your system:</p>

<ul>
  <li>CUDA Environment</li>
  <li><a href="http://torch.ch/">Torch-7</a></li>
  <li><a href="https://github.com/torchnet/torchnet">Torchnet</a></li>
  <li>Torch Libraries
    <ul>
      <li><a href="https://github.com/torch/image">image</a></li>
      <li><a href="https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md">hdf5</a></li>
      <li><a href="https://github.com/torch/nngraph">nngraph</a></li>
      <li><a href="https://github.com/torch/optim">optim</a></li>
      <li><a href="https://github.com/soumith/cudnn.torch">cudnn</a></li>
      <li><a href="https://github.com/torch/paths">paths</a></li>
      <li><a href="https://github.com/clementfarabet/lua---csv">csvigo</a></li>
      <li><a href="https://www.imagemagick.org/script/index.php">imagemagick</a></li>
    </ul>
  </li>
</ul>

<h2 id="cloning-repo">Cloning Repo</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/qureai/ultrasound-nerve-segmentation-using-torchnet.git
cd ultrasound-nerve-segmentation-using-torchnet
</code></pre></div></div>

<h2 id="dataset-generation">Dataset Generation</h2>

<p>The <a href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/data">dataset</a> consists of 5635 training images and their masks, and 5508 testing images.
The images are in tiff format, and to be able to load them into lua, we convert then to png format. So firstly we need to setup dataset so that</p>

<ul>
  <li>Train images are in <code class="language-plaintext highlighter-rouge">/path/to/train/data/images</code></li>
  <li>Train masks are in <code class="language-plaintext highlighter-rouge">/path/to/train/data/masks</code></li>
  <li>Test images are in <code class="language-plaintext highlighter-rouge">/path/to/test/data</code></li>
</ul>

<p>Now, go to each folder and run the following command, it will generate .png file for each .tif file in the folder. Be patient the procedure takes time.</p>

<p>``` mogrify -format png *.tif</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Now, we have all images in png format. To create datasets run the following command

</code></pre></div></div>
<p>th create_dataset.lua -train /path/to/train/data/images -trainOutput /path/to/train/data.h5 -test /path/to/test/data -testOutput /path/to/test/data.h5</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
This will package the dataset into HDF5 format, such that train images and masks of patient number `N` are in paths `/images_N` and `/masks_N` of the train HDF5 file respectively. The test images are in `/images` path of test HDF5 file generated.

## Model

We are using a slightly Modified [U-Net](https://arxiv.org/abs/1505.04597) with [Kaiming-He](https://arxiv.org/abs/1502.01852) initialization. The structure of U-Net generated using nngraph can be found [here](/assets/images/ultrasound_torchnet/U-Net.svg).
Source code to create this model is at `models/unet.lua`

&lt;p align="center"&gt;
    &lt;img src="/assets/images/ultrasound_torchnet/u-net-architecture.png" alt="U-Net Architecture"&gt;
    &lt;br&gt;
    &lt;small&gt; U-Net architecture &lt;/small&gt;
&lt;/p&gt;


## Training

You can start training right away by running

```bash
th main.lua [OPTIONS]
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Default value</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-dataset</code></td>
      <td><code class="language-plaintext highlighter-rouge">data/train.h5</code></td>
      <td>Path to training dataset to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-model</code></td>
      <td><code class="language-plaintext highlighter-rouge">models/unet.lua</code></td>
      <td>Path of the model to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-trainSize</code></td>
      <td>100</td>
      <td>Amount of data to be used for training, -1 if complete train data to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-valSize</code></td>
      <td>25</td>
      <td>Amount of data to be used for validation, -1 if complete validation to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-trainBatchSize</code></td>
      <td>64</td>
      <td>Size of batch size to be used for training</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-valBatchSize</code></td>
      <td>32</td>
      <td>Size of batch size to be used for validation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-savePath</code></td>
      <td><code class="language-plaintext highlighter-rouge">data/saved_models</code></td>
      <td>Path where models must be saved</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-optimMethod</code></td>
      <td><code class="language-plaintext highlighter-rouge">sgd</code></td>
      <td>Method to be used for training, can be sgd or adam</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-maxepoch</code></td>
      <td>250</td>
      <td>Maximum epochs for which training must be done</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-cvparam</code></td>
      <td>2</td>
      <td>Cross validation parameter</td>
    </tr>
  </tbody>
</table>

<h3 id="train-validation-data-split">Train Validation Data Split</h3>

<p>The images are given for each patient, and thus in the dataset we have 47 patients with each patient having 119 or 120 images. To assess the real performance of our model, we divide the dataset into train and validation based on patients and use 80-20 split. Thus, now question arises which patients to use for validation and which for training.</p>

<p>To solve this, we keep a parameter <code class="language-plaintext highlighter-rouge">-cvparam</code>, such that all patients with <code class="language-plaintext highlighter-rouge">patient_id%5==cvparam</code> are used in validation, else in training. Now out of these images, we select <code class="language-plaintext highlighter-rouge">-trainSize</code> number of images and <code class="language-plaintext highlighter-rouge">-valSize</code> number of images for training and validation respectively. This allows us to do cross validation easily.</p>

<h3 id="data-augmentation">Data Augmentation</h3>

<p>Data augmentation plays a vital role in any segmentation problem with limited dataset. Here we do on-the-fly data augmentation using modified <a href="https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua">Facebook’s resnet’s transformation file</a>. The image goes through following transformations:</p>

<ol>
  <li>Horizontal flip with probability 0.5</li>
  <li>Vertical flip with probability 0.5</li>
  <li>Rotation between -5 to 5 degrees with uniform probability</li>
  <li>Elastic transformations</li>
</ol>

<h3 id="constants">Constants</h3>

<p>We resize the image to <code class="language-plaintext highlighter-rouge">imgWidth X imgHeight</code> and then pass to our model.
For creating segmentation masks, we consider a pixel from the output to be a part of mask if <code class="language-plaintext highlighter-rouge">prob_pixel &gt; baseSegmentationProb</code> where <code class="language-plaintext highlighter-rouge">prob_pixel</code> is predicted probability that pixel is nerve.
One can define these values in <code class="language-plaintext highlighter-rouge">constants.lua</code> file.</p>

<p>While your model is training, you can look into how torchnet was used to create the training pipeline.</p>

<h2 id="using-torchnet">Using Torchnet</h2>

<p><a href="https://github.com/torchnet/torchnet">Torchnet</a> was introduced in second half of June to enable code re-use and to make writing code in Torch much more simple. It is basically a well structured implementation of the boilerplate code such as permutation for batches, training for loop and all such things, into a single library. In this project, we have used 4 major tools provided by torchnet</p>

<ol>
  <li>Datasets</li>
  <li>Dataset Iterators</li>
  <li>Engine</li>
  <li>Meters</li>
</ol>

<h3 id="datasets">Datasets</h3>

<p>Torchnet provides a abstract class <code class="language-plaintext highlighter-rouge">tnt.Dataset</code> and implementations of it to easily to easily concat, split, batch, resample etc. datasets. We use two of these implementations:</p>

<ul>
  <li><a href="https://github.com/torchnet/torchnet#tntlistdatasetself-list-load-path"><code class="language-plaintext highlighter-rouge">tnt.ListDataset</code></a>: Given a <code class="language-plaintext highlighter-rouge">list</code> and <code class="language-plaintext highlighter-rouge">load()</code> closure, ith sample of dataset will be returned by <code class="language-plaintext highlighter-rouge">load(list[i])</code></li>
  <li><a href="https://github.com/torchnet/torchnet#tntshuffledatasetself-dataset-size-replacement"><code class="language-plaintext highlighter-rouge">tnt.ShuffleDataset</code></a>: Given a <code class="language-plaintext highlighter-rouge">dataset</code> like above, it creates a new <code class="language-plaintext highlighter-rouge">Dataset</code> by shuffling it.</li>
</ul>

<p>For our model to generalize as it converges, providing a shuffled dataset on every epoch is an important strategy. So we load the data with <code class="language-plaintext highlighter-rouge">tnt.ListDataset</code> and then wrap it with <code class="language-plaintext highlighter-rouge">tnt.ShuffleDataset</code>:</p>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">local</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">tnt</span><span class="p">.</span><span class="n">ShuffleDataset</span><span class="p">{</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tnt</span><span class="p">.</span><span class="n">ListDataset</span><span class="p">{</span>
        <span class="n">list</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">#</span><span class="n">images</span><span class="p">):</span><span class="n">long</span><span class="p">(),</span>
        <span class="nb">load</span> <span class="o">=</span> <span class="k">function</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span> <span class="n">input</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">target</span> <span class="o">=</span> <span class="n">masks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="p">}</span>
        <span class="k">end</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This ensures that whenever you query the <code class="language-plaintext highlighter-rouge">dataset</code> for ith sample using <code class="language-plaintext highlighter-rouge">dataset:get(i)</code>, you get the image chosen at random from the dataset without replacement.</p>

<p align="center">
    <img src="/assets/images/ultrasound_torchnet/dataset.png" alt="Dataset" />
    <br />
    <small>Illustration of <code> dataset </code> </small>
</p>

<h3 id="dataset-iterator">Dataset Iterator</h3>

<p>While, it is easy to iterate over datasets using <code class="language-plaintext highlighter-rouge">dataset:get(i)</code> and a for loop, we can easily do on the fly and threaded data augmentation using <code class="language-plaintext highlighter-rouge">tnt.DatasetIterator</code></p>

<p>We call the iterator in every epoch, and it returns the batch over which training must be done. Before a batch is put for training, we must ensure that transformations for data augmentation take place and then batch is formed of the given size. Using shuffled dataset ensures that we get new order of data every epoch and thus batches are non-uniform across the epochs. <a href="https://github.com/torchnet/torchnet#tntbatchdatasetself-dataset-batchsize-perm-merge-policy"><code class="language-plaintext highlighter-rouge">tnt.BatchDataset</code></a> ensures that batches are formed from underlying images.</p>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="n">tnt</span><span class="p">.</span><span class="n">ParallelDatasetIterator</span><span class="p">{</span>
  <span class="n">nthread</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
  <span class="n">transform</span> <span class="o">=</span> <span class="n">GetTransforms</span><span class="p">(</span><span class="n">mode</span><span class="p">),</span> <span class="c1">--transforms for data augmentation</span>
  <span class="n">init</span> <span class="o">=</span> <span class="k">function</span><span class="p">()</span>
     <span class="n">tnt</span> <span class="o">=</span> <span class="nb">require</span> <span class="s1">'torchnet'</span>
  <span class="k">end</span><span class="p">,</span>
  <span class="n">closure</span> <span class="o">=</span> <span class="k">function</span><span class="p">()</span>
     <span class="k">return</span> <span class="n">tnt</span><span class="p">.</span><span class="n">BatchDataset</span><span class="p">{</span>
        <span class="n">batchsize</span> <span class="o">=</span> <span class="n">batchSize</span><span class="p">,</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span>
     <span class="p">}</span>
  <span class="k">end</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We use <a href="https://github.com/torchnet/torchnet#tntparalleldatasetiteratorself-init-closure-nthread-perm-filter-transform-ordered"><code class="language-plaintext highlighter-rouge">tnt.ParallelDatasetIterator</code></a> with transforms, which ensures that when the training is going for batch <code class="language-plaintext highlighter-rouge">n</code>, it will apply transforms on batch <code class="language-plaintext highlighter-rouge">n+1</code> in parallel and thus reducing the time for training.</p>

<p align="center">
    <img src="/assets/images/ultrasound_torchnet/dataset_iterator.png" alt="Dataset Iterator" />
</p>

<h3 id="engine">Engine</h3>

<p>From torch documentation,</p>

<blockquote>
  <p>In experimenting with different models and datasets, the underlying training procedure is often the same. The Engine module provides the boilerplate logic necessary for the training and testing of models. This might include conducting the interaction between model (nn.Module), tnt.DatasetIterators, nn.Criterions, and tnt.Meters.</p>
</blockquote>

<p>Engine is the main running core that will put your model into train. We use <a href="https://github.com/torchnet/torchnet#tntoptimengine">optim engine</a> which wraps the optimization functions of <a href="https://github.com/torch/optim">optim</a> package of torch. Engine has hooks attached with different events of training. We can define a callback function and attach to the hooks, hooks ensure that these functions are called at the end of event it is attached to. We use these hooks to update our meters, save model and print the statistics of the training.</p>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">engine</span><span class="p">:</span><span class="n">train</span><span class="p">{</span>
  <span class="n">network</span>   <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
  <span class="n">iterator</span>  <span class="o">=</span> <span class="n">getIterator</span><span class="p">(</span><span class="s1">'train'</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">trainDataset</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">trainBatchSize</span><span class="p">),</span>
  <span class="n">criterion</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">criterion</span><span class="p">,</span>
  <span class="n">optimMethod</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">optimMethod</span><span class="p">,</span>
  <span class="n">config</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">optimConfig</span><span class="p">,</span>
  <span class="n">maxepoch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">maxepoch</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Below is an example of the hook that we attach to on end epoch event. We validate the model, print the meters and save model.</p>

<div class="language-lua highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">local</span> <span class="n">onEndEpochHook</span> <span class="o">=</span> <span class="k">function</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
  <span class="n">state</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">self</span><span class="p">:</span><span class="n">test</span><span class="p">()</span>
  <span class="n">self</span><span class="p">:</span><span class="n">PrintMeters</span><span class="p">()</span>
  <span class="n">self</span><span class="p">:</span><span class="n">saveModels</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">state</code> supplied to hook function stores the current information about the training process, such as number of epochs done, model, criterion, etc.</p>

<h3 id="meters">Meters</h3>

<p>Again from torchnet’s <a href="https://github.com/torchnet/torchnet#meters">documentation</a>,</p>

<blockquote>
  <p>When training a model, you generally would like to measure how the model is performing. Specifically, you may want to measure the average processing time required per batch of data, the classification error or AUC of a classifier a validation set, or the precision@k of a retrieval model.</p>

  <p>Meters provide a standardized way to measure a range of different measures, which makes it easy to measure a wide range of properties of your models.</p>
</blockquote>

<p>We use <a href="https://github.com/torchnet/torchnet#tntaveragevaluemeterself"><code class="language-plaintext highlighter-rouge">tnt.AverageValueMeter</code></a> for all parameters we want to observe such as validation dice scrore, validation loss, training loss, training dice score, etc. . They are set to zero on beginning of every epoch, updated at the end of an iteration in an epoch and printed at the end of every epoch.</p>

<h2 id="submission">Submission</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>th generate_submission.lua [OPTIONS]
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Default value</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-dataset</code></td>
      <td><code class="language-plaintext highlighter-rouge">data/test.h5</code></td>
      <td>Path to dataset to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-model</code></td>
      <td><code class="language-plaintext highlighter-rouge">models/unet.t7</code></td>
      <td>Path of the model to be used</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-csv</code></td>
      <td><code class="language-plaintext highlighter-rouge">submission.csv</code></td>
      <td>Path of the csv to be generated</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">-testSize</code></td>
      <td>5508</td>
      <td>Number of images to be used for generating test data, must be &lt; 5508</td>
    </tr>
  </tbody>
</table>

<h2 id="results">Results</h2>

<p>The model takes about 3 min for each epoch on a Titan X GPU. Using adam for training we received score greater than 0.620 on the leaderboard while using SGD takes it to greater than 0.628. It takes about 7 min to generate the submission file. Rock On!!</p>

<p>Let us know if this was helpful and feel free to reach out to us through the forum.</p>
:ET